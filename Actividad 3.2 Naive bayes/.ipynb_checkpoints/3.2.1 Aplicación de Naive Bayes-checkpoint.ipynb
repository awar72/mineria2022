{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicación de Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos previos\n",
    "\n",
    "Es uno de los algoritmos más simples y poderosos para la clasificación basado en el Teorema de Bayes con una suposición de **independencia entre los predictores**. Naive Bayes es fácil de construir y particularmente útil para conjuntos de datos muy grandes.\n",
    "\n",
    "Naive Bayes asume que el efecto de una característica particular en una clase es independiente de otras características. Por ejemplo, un solicitante de préstamo es deseable o no dependiendo de sus ingresos, historial de préstamos y transacciones anteriores, edad y ubicación. Incluso si estas características son interdependientes, estas características se consideran de forma independiente. \n",
    "\n",
    "Esta suposición simplifica la computación, es por esa razón el nombre del algoritmo **Bayes ingenuo**. Formalmente, esta suposición se denomina **independencia condicional de clase**.\n",
    "\n",
    "$P(clase|Datos) = \\frac{P(Datos|clase)*P(clase)}{P(Datos)}$\n",
    "\n",
    "Donde:\n",
    "+ clase es una dalida en particular, en este caso será benigna\n",
    "\n",
    "+ datos son las características\n",
    "\n",
    "+ P(clase) se conoce como la **probabilidad anterior**. Es la probabilidad que ya se tiene\n",
    "\n",
    "+ P(Datos) se conoce como **probabilidad marginal**.\n",
    "\n",
    "+ P(clase|Datos) se conoce como **probabilidad posterior**. Es el resultado que se quiere encontrar\n",
    "\n",
    "+ P(Datos|clase) se conoce como **verosimilitud**\n",
    "\n",
    "### Cómo funciona\n",
    "\n",
    "En caso de que se tenga una sola característica, el clasificador Naive Bayes calcula la probabilidad de un evento en los siguientes pasos:\n",
    "\n",
    "+ 1 : calcular la probabilidad previa para las etiquetas de clase dadas.\n",
    "+ 2 : determinar la probabilidad de probabilidad con cada atributo para cada clase.\n",
    "+ 3 : poner estos valores en el teorema de Bayes y calcular la probabilidad posterior.\n",
    "+ 4 : ver qué clase tiene una probabilidad más alta, dado que la variable de entrada pertenece a la clase de probabilidad más alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set de datos\n",
    "\n",
    "Se usará un set de datos precargado en scikit-learn que tiene relación con datos de muestras de biopsias que pueden ser benignas o malignas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carga de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_breast_cancer()\n",
    "data_frame = pd.DataFrame(np.c_[dataset['data'], dataset['target']],\n",
    "                  columns= np.append(dataset['feature_names'], ['target']))\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de datos de las columnas\n",
    "data_frame.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisa la distribución de las observaciones respecto de la variable que se usará para la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_frame.groupby('target').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Gráfico de tortas del porcentaje de muestras benignas y mañignas\n",
    "# Contando las benignas\n",
    "clases = np.array([data_frame[data_frame.target == 0.0].shape[0], data_frame[data_frame.target == 1.0].shape[0]])\n",
    "\n",
    "# Creando las leyendas del grafico.\n",
    "labels = [ str(round(x * 1.0 / clases.sum() * 100.0, 2)) + '%'  for x in clases ]\n",
    "labels[0] = 'Benignas\\n ' + labels[0]\n",
    "labels[1] = 'Malignas\\n ' + labels[1]\n",
    "\n",
    "plt.pie(clases, labels=labels)\n",
    "plt.title('Porcentaje de muestras beningnas y malignas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios hasta este punto\n",
    "\n",
    "+ Hay 569 observaciones con 31 columnas, siendo una de ellas la columna 'target' que es la que indica si es o no es cáncer.\n",
    "+ Se observa que hay una distribución aceptable de muestras malignas y benignas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de distribución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "columnas = np.array(data_frame.columns)\n",
    "\n",
    "for col in columnas[:-1]:\n",
    "    fig, ax =plt.subplots(1, 2, figsize=(20, 6))\n",
    "    fig.suptitle(col, fontsize=18)\n",
    "    sns.distplot(data_frame[col], ax=ax[0], kde=False)\n",
    "    data_frame[[col]+['target']].plot.scatter(x=col, y='target', ax=ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona las variables\n",
    "X = data_frame.drop([\"target\"],axis=1)\n",
    "\n",
    "# Rescata la etiqueta\n",
    "y = data_frame.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el modelo\n",
    "\n",
    "### Ideas previas\n",
    "\n",
    "Se está tratando de elegir entre 2 clases: benigna o maligna; entonces una forma de tratar la decisión es calcular la tasa de probabilidades a posterior.\n",
    "\n",
    "Cuando se trabaja con Naive Bayes se debe escoger un **clasificador**. Uno de los tipos de clasificadores más populares es el **Gaussian Naive Bayes Classifier**. \n",
    "\n",
    "Hay otros clasificadores Bayesianos (https://scikit-learn.org/stable/modules/naive_bayes.html#). \n",
    "\n",
    "La fórmula del clasificador, usando 2 clases (benigna, maligna) y 2 características (solo para explicar la fórmula): mean_radius, mean_texture.\n",
    "\n",
    "$P(benigna|datos) = \\frac{P(benigna)*P(mean\\_radius|benigna)*P(mean\\_texture|benigna)}{P(datos)}$\n",
    "\n",
    "$P(maligna|datos) = \\frac{P(maligna)*P(mean\\_radius|maligna)*P(mean\\_texture|maligna)}{P(datos)}$\n",
    "\n",
    "donde:\n",
    "\n",
    "+ P(benigna) es la probabilidad que ya se tiene. Corresponde al número de veces que el valor de target = 0.0 en el conjunto de datos, dividido el total de observaciones. En este caso (como ya se revisó) es 212/569\n",
    "\n",
    "+ $P(mean\\_radius|benigna), P(mean\\_texture|benigna) $ es la verosimilitud.\n",
    "\n",
    "Los nombres Gaussian y Naive (ingenuo) de algoritmo vienen de dos suposiciones:\n",
    "\n",
    "+ Se asume que las características de la verosimilitud no estan correlacionada entre ellas. Esto seria que el promedio de radio y la etxtura promedio son independienets entre ellos. Dado que eso no siempre puede ser cierto y es una suposición ingenua es que aparece en el nombre **naive bayes**\n",
    "\n",
    "+ Se asume que el valor de las características (mean_radius, mean_texture y todas las que se quieran agregar) tendrán una distribución normal (gaussiana). Esto permite calcular cada parte p(mean_radius|benigna) usando la función de probabilidad de densidad normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de datos de entrenamiento y validación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Se separan los datos de \"train\" en entrenamiento y prueba para probar los algoritmos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 29)\n",
    "\n",
    "# Define el algoritmo a utilizar Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "modelo = GaussianNB()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Validación del modelo\n",
    "y_pred = modelo.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# Se calcula la precisión del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print('Precisión del modelo:', precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de resultados\n",
    "\n",
    "Desde la matriz de confusión:\n",
    "\n",
    "+ Se obtienen 107 datos predichos correctamente\n",
    "+ Se obtienen 7 datos erróneos\n",
    "\n",
    "### Acerca de la matriz de confusión\n",
    "\n",
    "La imagen muestra la estructura de la matriz de confusión:\n",
    "\n",
    "![Matriz de confusión](MatrizConfusion.png  \"Matriz de confusión\") \n",
    "\n",
    "Si se considera que una muestra se dice beningna si está etiquetada como no cancerígena y maligna en caso contrario. \n",
    "\n",
    "Considerando lo anterior esto se interpreta la matriz de la siguiente forma:\n",
    "\n",
    "+ 1. Muestra maligna y el modelo la clasificó como maligna (+) . Esto sería un verdadero positivo o VP .\n",
    "+ 2. Muestra maligna y el modelo lo clasificó como benigna (-) . Este seria un verdadero negativo o sea un VN.\n",
    "+ 3. Muestra benigna y el modelo lo clasificó como benigna (-) . Este seria un error tipo II o falso negativo o FN.\n",
    "+ 4. Muestra benigna y el modelo lo clasificó como maligna (+) . Este es un error tipo I, o falso positivo o FP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras medidas\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accurancy del modelo:',accuracy)\n",
    "\n",
    "print('F1 score del modelo:',f1_score(y_test, y_pred))\n",
    "print('Recall del modelo:',recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando un selector de columnas\n",
    "\n",
    "Para mejorar los resultados con este algoritmo. En vez de utilizar las 30 columnas de datos de entrada que se tienen, se va a utilizar una Clase de SkLearn llamada **SelectKBest** con la que seleccionaremos las 5 mejores características y usarán sólo esas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "X=data_frame.drop(['target'], axis=1)\n",
    "y=data_frame['target']\n",
    "\n",
    "best=SelectKBest(k=5)\n",
    "X_new = best.fit_transform(X, y)\n",
    "X_new.shape\n",
    "selected = best.get_support(indices=True)\n",
    "print(X.columns[selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "used_features =X.columns[selected]\n",
    "\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sb.heatmap(data_frame[used_features].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_frame[used_features]\n",
    "# Se separan los datos de \"train\" en entrenamiento y prueba para probar los algoritmos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 29)\n",
    "\n",
    "modelo_x = GaussianNB()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "modelo_x.fit(X_train, y_train)\n",
    "\n",
    "# Validación del modelo\n",
    "y_pred = modelo_x.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo\n",
    "matriz_x = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz_x)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
